Feature	SVD	NMF	LDA
Full Name	Singular Value Decomposition	Non-negative Matrix Factorization	Latent Dirichlet Allocation
Type	Matrix Factorization	Matrix Factorization	Probabilistic Generative Model
Input	Document-Term Matrix (TF-IDF)	Document-Term Matrix (TF-IDF)	Bag-of-Words
Output	Topics as combinations of terms	Topics as combinations of terms	Topics as distributions over words
Output Interpretation	Dense, orthogonal dimensions	Sparse, additive parts	Probabilistic word distributions
Topic Coherence	Moderate	High (better interpretability)	Often high, tunable
Scalability	Fast with SVD libraries	Moderate	Slower but parallelizable
Non-Negative Constraint	‚ùå No	‚úÖ Yes	‚úÖ (Probabilistic)
Topic Sparsity	‚ùå Low	‚úÖ High	‚úÖ High (depends on priors)
Probabilistic	‚ùå No	‚ùå No	‚úÖ Yes
Interpretability	Medium	High	High
Best For	Dimensionality Reduction	Parts-based Topic Modelling	Topic Discovery in Text Collections

1. SVD (Latent Semantic Analysis - LSA)

    Method: Factorizes the term-document matrix into 3 matrices using linear algebra.

    Use in NLP: LSA uses SVD on a TF-IDF matrix to identify latent concepts.

    Pros:

        Fast and simple.

        Good for dimensionality reduction.

    Cons:

        Not interpretable ‚Äî produces negative values.

        Topics are hard to label manually.

üî¨ 2. NMF

    Method: Factorizes the document-term matrix into two non-negative matrices (topics and weights).

    Interpretation: Each topic is an additive combination of words, making them intuitive.

    Pros:

        Produces sparse, interpretable topics.

        Easy to label and understand.

    Cons:

        Sensitive to initialization.

        Slower than SVD on large corpora.

üîç 3. LDA (Latent Dirichlet Allocation)

    Method: A probabilistic model that assumes documents are generated by a mixture of topics.

    Output: Probability distributions over words for topics, and over topics for documents.

    Pros:

        Rich theoretical foundation.

        Highly interpretable, tunable sparsity.

        Good for mixed-topic documents.

    Cons:

        Slower to train.

        Requires hyperparameter tuning.

        Sensitive to corpus size and document length.

üß† Which One to Use?
Goal	Recommended
Quick dimensionality reduction	SVD/LSA
Interpretability & sparsity	NMF
Probabilistic topic inference	LDA
Few documents, high quality	NMF or LDA
Large dataset with multiple topics	LDA
